---
title: "SURV727 Final Project"
author: "Akari Oya, Zhouer Wang"
format: html
---

# Github link: <https://github.com/ZuorW/SURV727.git>

```{r, include = FALSE}
library(tidyverse)
library(openmeteo)
library(gtrendsR)
```


# Question:
Are there any patterns in depression-related Google searches by weather in Michigan?

## Open Meteo API
For daylight duration
```{r}
# Load necessary package
library(openmeteo)

# see which weather variables we have data for (not a complete list)
weather_variables()

# Test code with one city first
weather_daylight <- weather_history("Detroit",
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "daylight_duration") # pulling for dayling duration

# Check data
str(weather_daylight)
head(weather_daylight) # daylight seems to be in seconds
```

```{r}
# Manipulate/clean dataset with one city first

# Rename columns for ease of reading
weather_daylight <- weather_daylight %>% 
  rename(daylight_sec = daily_daylight_duration)

# Add column for city
weather_daylight$city <- "Detroit"

# Add column for daylight by hour
weather_daylight$daylight_hrs <- weather_daylight$daylight_sec/ 3600
weather_daylight$year <- substr(weather_daylight$date, 1,4)

# Add weekly average daylight column so it matches gtrends
weather_daylight <- weather_daylight %>% 
  mutate(week = week(ymd(date))) %>% 
  group_by(week) %>% 
  mutate(weekavg_hrs = mean(daylight_hrs))

# Reorder columns
colnames(weather_daylight)
col_order <- c("date", "year", "week", "city", "daylight_hrs", "weekavg_hrs")
weather_daylight <- weather_daylight[, col_order]

head(weather_daylight)
```

```{r}
# Create loop to grab all cities of interest

# First, create list of cities of interest
cities <- c("Detroit", "Ann Arbor", "Lansing", "Grand Rapids", "Kalamazoo")

# Create object to store each city as a data frame
daylight_dataframes <- list()

# Use weather_history() to get historical weather data
for(i in cities) {
weather_daylight <- weather_history(i,
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "daylight_duration")

  weather_daylight <- weather_daylight %>% 
    rename(daylight_sec = daily_daylight_duration) #rename column
  weather_daylight$daylight_hrs <- weather_daylight$daylight_sec / 3600 #create daylight by hour column
  weather_daylight$city <- i # add city column
  weather_daylight$year <- substr(weather_daylight$date, 1,4) #create year column
  
  weather_daylight <- weather_daylight %>% 
    mutate(week = week(ymd(date))) %>% #add week column
    group_by(week) %>%  
    mutate(weekavg_hrs = mean(daylight_hrs)) #compute weekly average daylight hours

  weather_daylight <- weather_daylight[, col_order] #reorder columns
  daylight_dataframes[[i]] <- weather_daylight #add new data frame to list
}

head(daylight_dataframes)
```


Same as above but for snow fall:
```{r}
# Test code with one city first
weather_snow <- weather_history("Detroit",
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "snowfall_sum") # unit is inches

# Check data
str(weather_snow)
head(weather_snow)
```

```{r}
# Create loop to grab all cities of interest for snow fall

# First, create list of cities of interest
col_order_snow2 <- c("date", "week", "city", "daily_snow", "weekavg_snow")

# Create object to store each city as a data frame
snow_dataframes <- list()

# Use weather_history() to get historical weather data
for(i in cities) {
weather_snow <- weather_history(i,
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "snowfall_sum")

  weather_snow <- weather_snow %>% 
    rename(daily_snow = daily_snowfall_sum) #rename column
  weather_snow$city <- i # add city column
  weather_snow$year <- substr(weather_snow$date, 1,4) #create year column
  
  weather_snow <- weather_snow %>% 
    mutate(week = week(ymd(date))) %>% #add week column
    group_by(week) %>%  
    mutate(weekavg_snow = mean(daily_snow)) #compute weekly average daylight hours

  weather_snow <- weather_snow[, col_order_snow2] #reorder columns
  snow_dataframes[[i]] <- weather_snow #add new data frame to list
}

head(snow_dataframes)
```

Combining city data
```{r}
# Combine all city dataframes into one long dataframe
daylight_long <- bind_rows(daylight_dataframes)
head(daylight_long)

# Combine all city dataframes into one long dataframe
snow_long <- bind_rows(snow_dataframes)
head(snow_long)
```


Combining daylight and snowfall data
```{r}
weather <- merge(daylight_long, snow_long, by = c("date", "week", "city"))
head(weather)
colnames(weather) 
```


Making plots for weather data
```{r}
ggplot(data = weather, aes(x= date)) +
  geom_line(aes(y = daily_snow, color = city))

ggplot(data = weather, aes(x = date)) +
  geom_line(aes(y = daylight_hrs, color = city)) +
   scale_x_date(limits = as.Date(c("2018-01-01","2018-12-31"))) +
   ggtitle("Average Weekly Daylight in 2018") + xlab("Week") + ylab("Daylight (hours)")

ggplot(data = weather, aes(x = date)) +
  geom_line(aes(y = daylight_hrs, color = city), show.legend = FALSE) +
  ggtitle("Average Weekly Daylight: 2018 - 2023") + xlab("Year") + ylab("Daylight (hours)")
```
  
```{r}
ggplot(data = weather, aes(x = date)) +
  geom_line(aes(y = weekavg_snow, color = city)) +
   scale_x_date(limits = as.Date(c("2018-01-01","2018-12-31"))) +
   ggtitle("Average Weekly Snowfall in 2018") + xlab("Week") + ylab("Snowfall (mm)")
```

```{r}
weather %>% 
  group_by(year) %>% 
  summarise(mean_daily_snow = mean(daily_snow),
            min_daily_snow = min(daily_snow),
            max_daily_snow = max(daily_snow))
weather %>% 
  group_by(year, city) %>% 
  summarise(mean_daily_snow = mean(daily_snow),
            min_daily_snow = min(daily_snow),
            max_daily_snow = max(daily_snow))

weather %>% 
  group_by(year, city) %>% 
  summarise(mean_daylight = mean(daylight_hrs),
            min_daylight = min(daylight_hrs),
            max_daylight = max(daylight_hrs))
```




Google trends data
```{r}
mich.trends <- read.csv("/Users/akarioya/temp/SURV727/Final Project/17_23_SearchFreqeuncies.csv", skip = 2) #allows third row to be column names
colnames(mich.trends)
```

```{r}
# Clean gtrends whole Michigan data to match weather data
# Reorder date
mich.trends <- mich.trends %>% 
  mutate(date = as.Date(Week, format = "%m/%d/%y")) %>% 
  select(-c(Week)) %>% #remove old Week column
  mutate(year = substr(date, 1,4)) %>% 
  rename(light_therapy = light.therapy...Michigan.) %>% 
  rename(winter_blues = winter.blues...Michigan.) %>% 
  rename(SAD = Seasonal.affective.disorder...Michigan.) %>% 
  relocate(date, .before = light_therapy) #reorder columns

# Add week column that indicate number of weeks in year
mich.trends <- mich.trends %>% 
  mutate(week = week(ymd(date))) %>% 
  group_by(week)

head(mich.trends)
```

Combining whole Michigan gtrends data to weather data
```{r}
weather_nd <- weather[, -1] #remove date column so data can merge smoothly

# Merge datasets based on variables
mich_df <- merge(mich.trends, weather_nd, by = c("week", "year"))
head(mich_df)
colnames(mich_df) 
```

Creating interactive visualization (not useful right now because of missing city data in gtrends)
```{r}
# Add latitude and longitude conditionally based on the city
mich_df$latitude <- ifelse(mich_df$city == "Detroit", 42.3314,
                            ifelse(mich_df$city == "Ann Arbor", 42.2808,
                                   ifelse(mich_df$city == "Lansing", 42.7325,
                                          ifelse(mich_df$city == "Grand Rapids", 42.9634,
                                                 ifelse(mich_df$city == "Kalamazoo", 42.2917, NA)
                                          )
                                   )
                            )
)

mich_df$longitude <- ifelse(mich_df$city == "Detroit", -83.0458,
                             ifelse(mich_df$city == "Ann Arbor", -83.7430,
                                    ifelse(mich_df$city == "Lansing", -84.5555,
                                           ifelse(mich_df$city == "Grand Rapids", -85.6681,
                                                  ifelse(mich_df$city == "Kalamazoo", -85.5872, NA)
                                           )
                                    )
                             )
)
```

Trying Shiny app
```{r}
library(shiny)
library(rsconnect)
library(leaflet)

# Shiny UI
ui <- fluidPage(
  titlePanel("Interactive Map"),
  leafletOutput("map")
)

# Server
server <- function(input, output) {
  
  output$map <- renderLeaflet({
    leaflet() %>%
      addTiles() %>%
      setView(lng = -83.7430, lat = 44.3148, zoom = 7) %>%  # Coordinates for Michigan
      addMarkers(data = mich_df, ~longitude, ~latitude,
                 popup = paste("Date: ", mich_df$date, "<br>",
                               "City: ", mich_df$city, "<br>",
                               "Weather: ", mich_df$average_daily_daylight, " hours of daylight, ",
                               mich_df$average_daily_snow, " inches of snow", "<br>",
                               "Trends: Light Therapy - ", mich_df$light_therapy, ", ",
                               "SAD - ", mich_df$seasonal_affective_disorder, ", ",
                               "Winter Blues - ", mich_df$winter_blues))
  })
}

shinyApp(ui, server)

```





For Google Trends data
```{r}
# Query keywords
library(gtrendsR)
SADtrends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI", 
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
plot(SADtrends)

# Grab each dataset and save as CSV in case we have issues querying again
# interest by city
trends_city <- SADtrends$interest_by_city
write.csv(rest_city, "SADtrends_interest_by_city.csv")

# interest over time
trends_time <- SADtrends$interest_over_time
write.csv(trends_time, "SADtrends_interest_over_time.csv")

# interest by dma (seems like certain big cities)
trends_dma <- SADtrends$interest_by_dma
write.csv(trends_dma, "SADtrends_interest_by_dma.csv")

# interest by dma (seems like certain big cities)
trends_related <- SADtrends$related_queries
write.csv(trends_related, "SADtrends_related_queries.csv")
```


Examining interest over city data
```{r}
# Reshape data so each keyword becomes a column and each row is a city
city_ranking <- trends_city %>%
  pivot_wider(names_from = keyword, 
              values_from = hits)

# See data sorted by hits
head(sort(city_ranking$`light therapy`, decreasing = TRUE))
head(sort(city_ranking$`seasonal affective disorder`, decreasing = TRUE)) # this column seems to be the only one with data points
head(sort(city_ranking$`winter blues`, decreasing = TRUE))
```


Interest by city data above does not seem to have a lot of data points, so we try another method: query each major city separately.
There is no manual for which geo code to use for Michigan cities, but the URL on the gtrends website after selecting the filters displays a geo code. So we used this to see if it will pick up more specific data. (For example: https://trends.google.com/trends/explore?date=2017-12-01%202023-12-01&*geo=US-MI*-540&q=light%20therapy&hl=en-US)
```{r}
detroit_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-505", # geo code for Detroit from gtrends website URL 
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
detroit_trends <- as_tibble(detroit_trends$interest_over_time) 
write.csv(detroit_trends, "SADtrends_Detroit.csv") # save as csv

head(detroit_trends$interest_over_time)
write.csv(detroit_trends$interest_over_time, "SADtrends_Detroit.csv") # save as csv
```

Repeat with other cities (creating a loop gave me the connection error message)
```{r}
# List of city geo codes taken from gtrends URL
# Lansing = "US-MI-551"
# Alpena = "US-MI-583" - did not have any data, excluded
# Flint-Saginaw-BayCity = "US-MI-513"
# GrandRapids-Kalamazoo-BattleCreek = "US-MI-563"
# Marquette = "US-MI-553"
# Traverse City-Cadillac = "US-MI-540"

lansing_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-551",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
lansing_trends <- as_tibble(lansing_trends$interest_over_time) 
write.csv(lansing_trends, "SADtrends_Lansing.csv") # save as csv

flint_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-513",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
flint_trends <- as_tibble(flint_trends$interest_over_time)
write.csv(flint_trends, "SADtrends_Flint-Saginaw-BayCity.csv")

gr_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-563",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
gr_trends <- as_tibble(gr_trends$interest_over_time)
write.csv(gr_trends, "SADtrends_GrandRapids-Kalamazoo-BattleCreek.csv") # save as csv

marq_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-553",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
marq_trends <- as_tibble(marq_trends$interest_over_time)
write.csv(marq_trends, "SADtrends_Marquette.csv") # save as csv

trav_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-540",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
trav_trends <- as_tibble(trav_trends$interest_over_time)
write.csv(trav_trends, "SADtrends_Traverse City-Cadillac.csv") # save as csv

# Combine all into one dataset
SAD_bycity <- bind_rows(detroit_trends, lansing_trends, alpena_trends, flint_trends, gr_trends, marq_trends, trav_trends)
summary(SAD_bycity)
write.csv(SAD_bycity, "SADtrends_bycity.csv") # save as csv
```


```{r}
# Clean data
SAD_bycity_grp <- SAD_bycity %>% 
  mutate(date = as.Date(date, format = "%Y-%m/%d")) %>% 
  select(-c(time, gprop, category, interest_by_city)) %>% #remove unnecessary variables
  mutate(year = substr(date, 1,4)) %>% #create year variable
  mutate(month = substr(date,6,7)) %>% #create month variable  
  group_by(date, geo, year, month, keyword) %>%
  summarise(hits = sum(hits)) %>% 
  ungroup() %>% 
  rename(city = geo)

# Recode city values
SAD_bycity_grp$city <- recode(SAD_bycity_grp$city, 
                              "US-MI-505" = "Detroit",
                              "US-MI-551" = "Lansing",
                              "US-MI-513" = "Flint-Saginaw-Bay City",
                              "US-MI-563" = "Grand Rapids-Kalamazoo-Battle Creek",
                              "US-MI-553" = "Marquette",
                              "US-MI-540" = "Traverse City-Cadillac")

# Make keywords into columns with hits as values
SAD_bycity_grp <- SAD_bycity_grp %>%
  spread(key = keyword, value = hits)
head(SAD_bycity_grp)
```

Plots
```{r}
SAD_bycity_grp %>% ggplot(aes(x= date)) +
  geom_line(aes(y = `light therapy`, color = "Light Therapy"), linewidth = 1.2)+
  geom_line(aes(y = `winter blues`, color = "Winter Blues"), linewidth = 1.2) +
  geom_line(aes(y = `seasonal affective disorder`, color = "Seasonal Affective Disorder"), linewidth = 1.2) + 
  scale_x_date(limits = as.Date(c("2018-01-01","2018-12-31"))) +
  ggtitle("Search hits in 2018") + xlab("Month") + ylab("Search hits")
```



From Zhuoer's code
```{r}
# head(trends)
trends$Week = as.Date(trends$Week, "%m/%d/%y") # convert original data to a R readable format

ggplot(data = trends, aes(x= Week, group = 1))+
  geom_line(aes(y =light_therapy, color = "Light Therapy"), size =1.2)+
  geom_line(aes(y =winter_blues, color = "Winter Blues"),size =1.2) +
  geom_line(aes(y =Seasonal_affective_disorder, color = "Seasonal Affective Disorder"), size =1.2)
# we could use this plot (A) to see the overal trend across years

# then zoom to a subsection of A to see the data from 2017, plot (B)
#head(X17)
X17$Week = as.Date(X17$Week, "%m/%d/%y") # use this to transform the date to R readable ones

ggplot(data = X17, aes(x= Week, group = 1))+
  geom_line(aes(y =Seasonal_affective_disorder, color = "Seasonal Affective Disorder"), size =1.2) +
  geom_line(aes(y =winter_blues, color = "Winter Blues"),size =1.2) +
  geom_line(aes(y =light_therapy, color = "Light Therapy"), size =1.2) +
  theme(axis.text.x = element_text(size=10))
```


< Draft >
Data: What is the dataset you are obtaining? How did you obtain it? What are the characteristics of the dataset?

  Historical weather data was obtained through the publicly available Open-Meteo API (https://open-meteo.com).  We used the R package "openmeteo" to query daily total snowfall and daily daylight duration data between December 1, 2017 to December 1, 2023 from the following cities to represent different parts of Michigan (excluding the Upper Peninsula): Ann Arbor, Detroit, Lansing, Grand Rapids, and Kalamazoo. Each row in the queried dataset is a day of the year with each day's weather variables as columns. The main variables of interest are snowfall and daylight duration. Snowfall is a numeric variable indicating the total amount of snowfall in millimeters for each day. The average snowfall between 2018 and 2023 was 0.26 millimeters with a maximum of 25.55 millimeters. Daylight duration is a numeric variable representing the amount of seconds that daylight was present in each day. We converted the unit into hours, and daylight duration ranged from 9.00 to 15.36 hours with a mean of 12.24 hours across all years and cities.  The average daily daylight did not varied very little by year; while there was slightly more variation in snowfall by year, it did not vary by more than 0.06 millimeters.
  In order to capture SAD-related indicators over time, Google search frequency trends data was obtained through the R package "gtrendsR" for December 1, 2017 to December 1, 2023 from the following cities to represent different parts of Michigan (excluding the Upper Peninsula): Ann Arbor, Detroit, Lansing, Grand Rapids, and Kalamazoo. rows...
  
  

Conclusion: What is your conclusion based on your analysis? What are possible limitations of your analysis?

our conclusion...

  The current analysis has some limitations. One concern is unknown confounding variables that may play a large role in the relationship between daylight, snowfall, and changes in the public's interest of SAD-related topics reflected by Google search trends. Although some research suggesting that vitamin D synthesis in the body from sunlight affects or improves one's mood has gained popularity in recent years, supporting our analysis of Google search trends and weather data, the intent behind users' searches are not clear. Furthermore, higher or lower search hits do not directly reflect the actual number of diagnoses or people experiencing SAD-related symptoms. Thus, we cannot make strong or holistic conclusions about the link between weather and increase in SAD interest or diagnoses. 
  A second limitation is the nature of the Google trends data. Search trend hits were aggregated by week while hourly data was available for weather. Higher granulated data of search hits may reveal different patterns. Moreover, search hit frequencies were low for many cities and days which lead to missing data. Different sets of keywords may have better success in gathering more data and uncovering different information, but the task of choosing the appropriate keywords is challenging. Since mood is a factor in the link between weather and SAD, an additional analysis such as a sentiment analysis of Internet users' mood over the year may provide more nuance in our conclusions.
  

```{r}
weather %>% 
  group_by(year) %>% 
  summarise(daylight = mean(daylight_hrs),
            snow = mean(daily_snow))

```










