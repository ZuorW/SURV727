---
title: "SURV727 Final Project"
author: "Akari Oya, Zhouer Wang"
format: html
---

# Github link: <https://github.com/ZuorW/SURV727.git>

```{r, include = FALSE}
library(tidyverse)
```


# Question:
Are there any patterns in depression-related Google searches by weather in Michigan?

## Open Meteo API
For daylight duration
```{r}
# Load necessary package
library(openmeteo)

# see which weather variables we have data for (not a complete list)
weather_variables()

# Test code with one city first
weather_daylight <- weather_history("Detroit",
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "daylight_duration") # pulling for dayling duration

# Check data
str(weather_daylight)
head(weather_daylight) # daylight seems to be in seconds
```

```{r}
# Manipulate/clean dataset with one city first

# Rename columns for ease of reading
weather_daylight <- weather_daylight %>% 
  rename(daylight_sec = daily_daylight_duration)

# Add column for city
weather_daylight$city <- "Detroit"

# Add column for daylight by hour
weather_daylight$daylight_hrs <- weather_daylight$daylight_sec/ 3600
weather_daylight$year <- substr(weather_daylight$date, 1,4)

# Add weekly average daylight column so it matches gtrends
weather_daylight <- weather_daylight %>% 
  mutate(week = week(ymd(date))) %>% 
  group_by(week) %>% 
  mutate(weekavg_hrs = mean(daylight_hrs))

# Reorder columns
colnames(weather_daylight)
col_order <- c("date", "year", "week", "city", "daylight_hrs", "weekavg_hrs")
weather_daylight <- weather_daylight[, col_order]

head(weather_daylight)
```

```{r}
# Create loop to grab all cities of interest

# First, create list of cities of interest
cities <- c("Detroit", "Ann Arbor", "Lansing", "Grand Rapids", "Kalamazoo")

# Create object to store each city as a data frame
daylight_dataframes <- list()

# Use weather_history() to get historical weather data
for(i in cities) {
weather_daylight <- weather_history(i,
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "daylight_duration")

  weather_daylight <- weather_daylight %>% 
    rename(daylight_sec = daily_daylight_duration) #rename column
  weather_daylight$daylight_hrs <- weather_daylight$daylight_sec / 3600 #create daylight by hour column
  weather_daylight$city <- i # add city column
  weather_daylight$year <- substr(weather_daylight$date, 1,4) #create year column
  
  weather_daylight <- weather_daylight %>% 
    mutate(week = week(ymd(date))) %>% #add week column
    group_by(week) %>%  
    mutate(weekavg_hrs = mean(daylight_hrs)) #compute weekly average daylight hours

  weather_daylight <- weather_daylight[, col_order] #reorder columns
  daylight_dataframes[[i]] <- weather_daylight #add new data frame to list
}

head(daylight_dataframes)
```


Same as above but for snow fall:
```{r}
# Test code with one city first
weather_snow <- weather_history("Detroit",
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "snowfall_sum") # unit is inches

# Check data
str(weather_snow)
head(weather_snow)
```

```{r}
# Create loop to grab all cities of interest for snow fall

# First, create list of cities of interest
col_order_snow2 <- c("date", "week", "city", "daily_snow", "weekavg_snow")

# Create object to store each city as a data frame
snow_dataframes <- list()

# Use weather_history() to get historical weather data
for(i in cities) {
weather_snow <- weather_history(i,
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "snowfall_sum")

  weather_snow <- weather_snow %>% 
    rename(daily_snow = daily_snowfall_sum) #rename column
  weather_snow$city <- i # add city column
  weather_snow$year <- substr(weather_snow$date, 1,4) #create year column
  
  weather_snow <- weather_snow %>% 
    mutate(week = week(ymd(date))) %>% #add week column
    group_by(week) %>%  
    mutate(weekavg_snow = mean(daily_snow)) #compute weekly average daylight hours

  weather_snow <- weather_snow[, col_order_snow2] #reorder columns
  snow_dataframes[[i]] <- weather_snow #add new data frame to list
}

head(snow_dataframes)
```

Combining city data
```{r}
# Combine all city dataframes into one long dataframe
daylight_long <- bind_rows(daylight_dataframes)
head(daylight_long)

# Combine all city dataframes into one long dataframe
snow_long <- bind_rows(snow_dataframes)
head(snow_long)
```


Combining daylight and snowfall data
```{r}
weather <- merge(daylight_long, snow_long, by = c("date", "week", "city"))
head(weather)
colnames(weather) 
```


Making plots for weather data
```{r}
ggplot(data = weather, aes(x= date)) +
  geom_line(aes(y = daily_snow, color = city))

ggplot(data = weather, aes(x = date)) +
  geom_line(aes(y = daylight_hrs, color = city)) +
   scale_x_date(limits = as.Date(c("2018-01-01","2018-12-31"))) +
   ggtitle("Average Weekly Daylight in 2018") + xlab("Week") + ylab("Daylight (hours)")

ggplot(data = weather, aes(x = date)) +
  geom_line(aes(y = daylight_hrs, color = city), show.legend = FALSE) +
  ggtitle("Average Weekly Daylight: 2018 - 2023") + xlab("Year") + ylab("Daylight (hours)")
```
  
```{r}
ggplot(data = weather, aes(x = date)) +
  geom_line(aes(y = weekavg_snow, color = city)) +
   scale_x_date(limits = as.Date(c("2018-01-01","2018-12-31"))) +
   ggtitle("Average Weekly Snowfall in 2018") + xlab("Week") + ylab("Snowfall (mm)")
```

```{r}
weather %>% 
  group_by(year) %>% 
  summarise(mean_daily_snow = mean(daily_snow),
            min_daily_snow = min(daily_snow),
            max_daily_snow = max(daily_snow))
weather %>% 
  group_by(year, city) %>% 
  summarise(mean_daily_snow = mean(daily_snow),
            min_daily_snow = min(daily_snow),
            max_daily_snow = max(daily_snow))

weather %>% 
  group_by(year, city) %>% 
  summarise(mean_daylight = mean(daylight_hrs),
            min_daylight = min(daylight_hrs),
            max_daylight = max(daylight_hrs))
```




Google trends data
```{r}
mich.trends <- read.csv("/Users/akarioya/temp/SURV727/Final Project/17_23_SearchFreqeuncies.csv", skip = 2) #allows third row to be column names
colnames(mich.trends)
```

```{r}
# Clean gtrends whole Michigan data to match weather data
# Reorder date
mich.trends <- mich.trends %>% 
  mutate(date = as.Date(Week, format = "%m/%d/%y")) %>% 
  select(-c(Week)) %>% #remove old Week column
  mutate(year = substr(date, 1,4)) %>% 
  rename(light_therapy = light.therapy...Michigan.) %>% 
  rename(winter_blues = winter.blues...Michigan.) %>% 
  rename(SAD = Seasonal.affective.disorder...Michigan.) %>% 
  relocate(date, .before = light_therapy) #reorder columns

# Add week column that indicate number of weeks in year
mich.trends <- mich.trends %>% 
  mutate(week = week(ymd(date))) %>% 
  group_by(week)

head(mich.trends)
```

Combining whole Michigan gtrends data to weather data
```{r}
weather_nd <- weather[, -1] #remove date column so data can merge smoothly

# Merge datasets based on variables
mich_df <- merge(mich.trends, weather_nd, by = c("week", "year"))
head(mich_df)
colnames(mich_df) 
```

Creating interactive visualization (not useful right now because of missing city data in gtrends)
```{r}


# Add latitude and longitude conditionally based on the city
mich_df$latitude <- ifelse(mich_df$city == "Detroit", 42.3314,
                            ifelse(mich_df$city == "Ann Arbor", 42.2808,
                                   ifelse(mich_df$city == "Lansing", 42.7325,
                                          ifelse(mich_df$city == "Grand Rapids", 42.9634,
                                                 ifelse(mich_df$city == "Kalamazoo", 42.2917, NA)
                                          )
                                   )
                            )
)

mich_df$longitude <- ifelse(mich_df$city == "Detroit", -83.0458,
                             ifelse(mich_df$city == "Ann Arbor", -83.7430,
                                    ifelse(mich_df$city == "Lansing", -84.5555,
                                           ifelse(mich_df$city == "Grand Rapids", -85.6681,
                                                  ifelse(mich_df$city == "Kalamazoo", -85.5872, NA)
                                           )
                                    )
                             )
)
```

Trying Shiny app
```{r}
library(shiny)
library(rsconnect)
library(leaflet)

# Shiny UI
ui <- fluidPage(
  titlePanel("Interactive Map"),
  leafletOutput("map")
)

# Server
server <- function(input, output) {
  
  output$map <- renderLeaflet({
    leaflet() %>%
      addTiles() %>%
      setView(lng = -83.7430, lat = 44.3148, zoom = 7) %>%  # Coordinates for Michigan
      addMarkers(data = mich_df, ~longitude, ~latitude,
                 popup = paste("Date: ", mich_df$date, "<br>",
                               "City: ", mich_df$city, "<br>",
                               "Weather: ", mich_df$average_daily_daylight, " hours of daylight, ",
                               mich_df$average_daily_snow, " inches of snow", "<br>",
                               "Trends: Light Therapy - ", mich_df$light_therapy, ", ",
                               "SAD - ", mich_df$seasonal_affective_disorder, ", ",
                               "Winter Blues - ", mich_df$winter_blues))
  })
}

shinyApp(ui, server)

```





For Google Trends data
```{r}
# Query keywords
library(gtrendsR)
SADtrends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI", 
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
plot(SADtrends)

# Grab each dataset and save as CSV in case we have issues querying again
# interest by city
trends_city <- SADtrends$interest_by_city
write.csv(rest_city, "SADtrends_interest_by_city.csv")

# interest over time
trends_time <- SADtrends$interest_over_time
write.csv(trends_time, "SADtrends_interest_over_time.csv")

# interest by dma (seems like certain big cities)
trends_dma <- SADtrends$interest_by_dma
write.csv(trends_dma, "SADtrends_interest_by_dma.csv")

# interest by dma (seems like certain big cities)
trends_related <- SADtrends$related_queries
write.csv(trends_related, "SADtrends_related_queries.csv")
```


Examining interest over city data
```{r}
# Reshape data so each keyword becomes a column and each row is a city
city_ranking <- trends_city %>%
  pivot_wider(names_from = keyword, 
              values_from = hits)

# See data sorted by hits
head(sort(city_ranking$`light therapy`, decreasing = TRUE))
head(sort(city_ranking$`seasonal affective disorder`, decreasing = TRUE)) # this column seems to be the only one with data points
head(sort(city_ranking$`winter blues`, decreasing = TRUE))
```


Interest by city data above does not seem to have a lot of data points, so we try another method: query each major city separately.
There is no manual for which geo code to use for Michigan cities, but the URL on the gtrends website after selecting the filters displays a geo code. So we used this to see if it will pick up more specific data.
```{r}
detroit_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-505", # geo code for Detroit from gtrends website URL 
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)

head(detroit_trends) #geo code seems to include Detroit and surrounding South East Michigan cities
head(detroit_trends$interest_over_time)
write.csv(SADtrends$interest_over_time, "SADtrends_Detroit.csv") # save as csv
```

Repeat with other cities
```{r}
g_cities <- c("US-MI-505", "US-MI-551", "US-MI-583","US-MI-513", "US-MI-563", "US-MI-540")
# g_cities (in order) = Detroit, Lansing, Alpena, Flint-Saginaw-BayCity, GrandRapids-Kalamazoo-Battle Creek, Marquette, Traverse City-Cadillac

for(i in g_cities) {
lansing_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = i,  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)

head(lansing_trends) #geo code seems to include Detroit and surrounding South East Michigan cities
head(lansing_trends$interest_over_time)
}
```


From Zhouer's code
```{r}
# head(trends)
trends$Week = as.Date(trends$Week, "%m/%d/%y") # convert original data to a R readable format

ggplot(data = trends, aes(x= Week, group = 1))+
  geom_line(aes(y =light_therapy, color = "Light Therapy"), size =1.2)+
  geom_line(aes(y =winter_blues, color = "Winter Blues"),size =1.2) +
  geom_line(aes(y =Seasonal_affective_disorder, color = "Seasonal Affective Disorder"), size =1.2)
# we could use this plot (A) to see the overal trend across years

# then zoom to a subsection of A to see the data from 2017, plot (B)
#head(X17)
X17$Week = as.Date(X17$Week, "%m/%d/%y") # use this to transform the date to R readable ones

ggplot(data = X17, aes(x= Week, group = 1))+
  geom_line(aes(y =Seasonal_affective_disorder, color = "Seasonal Affective Disorder"), size =1.2) +
  geom_line(aes(y =winter_blues, color = "Winter Blues"),size =1.2) +
  geom_line(aes(y =light_therapy, color = "Light Therapy"), size =1.2) +
  theme(axis.text.x = element_text(size=10))
```
