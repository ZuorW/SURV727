---
title: "SURV727 Final Project Report"
author: "Akari Oya, Zhouer Wang"
format: pdf
bibliography: ref.bib
---

# Github link: <https://github.com/ZuorW/SURV727.git>

```{r, include = FALSE}
library(tidyverse)
library(openmeteo)
library(gtrendsR)
```

## Introduction:

### Literature Revieiw

Seasonal Affective Disorder (SAD) is a form of depression that typically appears during fall and winter, as the amount of natural sunlight decreases. This mood disorder affects millions of people across the globe, with varying degrees of severity. It is associated with a range of symptoms, including low energy, sleep problems, and even suicidal ideation in more severe cases [@PsychiatryOrg]. Remarkably, populations residing in the northern regions, such as Michigan, where winters are acute and sunlight is often sparse, observe a higher prevalence of SAD.

One of the most notable exacerbating factors for SAD is weather, specifically less sunlight and increased snowfall. Prior studies have suggested a possible correlation, with many individuals reporting a decline in their moods [@Postolache2011] or mental health during periods of increased snowfall or reduced sunlight [@Langer2023]. Hence, understanding the relationship between weather patterns and SAD prevalence could yield meaningful insights for health professionals, guiding the development of targeted interventions to mitigate the impact of SAD.

While there is a body of literature exploring different facets of SAD, there is a paucity of research specifically focused on assessing the relationship between weather patterns and SAD. The novel approach of examining depression-related Google search terms in relation to weather conditions in Michigan further emphasizes the uniqueness and significance of our study.

### Research Question:

Are there any patterns in depression-related Google searches by weather in Michigan?

## Data

To understand potential patterns in depression-related Google searches by weather in Michigan, we collected weather data and Google search frequencies for depression-related search terms.

### Open Meteo API

Historical weather data was obtained through the publicly available Open-Meteo API (https://open-meteo.com). We used the R package "openmeteo" to query daily total snowfall and daily daylight duration data between December 1, 2017 to December 1, 2023 from the following cities to represent different parts of Michigan (excluding the Upper Peninsula): Ann Arbor, Detroit, Lansing, Grand Rapids, and Kalamazoo. Each row in the queried dataset is a day of the year with each day's weather variables as columns. The main variables of interest are snowfall and daylight duration. Daylight duration is a numeric variable representing the amount of seconds that daylight was present in each day. We converted the unit into hours, and daylight duration ranged from 9.00 to 15.36 hours with a mean of 12.24 hours across all years and cities.

```{r, include = FALSE}
# For daylight duration

# Load necessary package
library(openmeteo)

# see which weather variables we have data for (not a complete list)
weather_variables()

# Test code with one city first
weather_daylight <- weather_history("Detroit",
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "daylight_duration") # pulling for dayling duration

# Check data
str(weather_daylight)
head(weather_daylight) # unit seems in seconds
```

```{r, include = FALSE}
# Manipulate/clean dataset with one city first

# Rename columns for ease of reading
weather_daylight <- weather_daylight %>% 
  rename(daylight_sec = daily_daylight_duration)

# Add column for city
weather_daylight$city <- "Detroit"

# Add column for daylight by hour
weather_daylight$daylight_hrs <- weather_daylight$daylight_sec/ 3600
weather_daylight$year <- substr(weather_daylight$date, 1,4)

# Add weekly average daylight column so it matches gtrends
weather_daylight <- weather_daylight %>% 
  mutate(week = week(ymd(date))) %>% 
  group_by(week) %>% 
  mutate(weekavg_hrs = mean(daylight_hrs))

# Reorder columns
colnames(weather_daylight)
col_order <- c("date", "year", "week", "city", "daylight_hrs", "weekavg_hrs")
weather_daylight <- weather_daylight[, col_order]

head(weather_daylight)
```

```{r, include = FALSE}
# Create loop to grab all cities of interest

# First, create list of cities of interest
cities <- c("Detroit", "Ann Arbor", "Lansing", "Grand Rapids", "Kalamazoo")

# Create object to store each city as a data frame
daylight_dataframes <- list()

# Use weather_history() to get historical weather data
for(i in cities) {
weather_daylight <- weather_history(i,
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "daylight_duration")

  weather_daylight <- weather_daylight %>% 
    rename(daylight_sec = daily_daylight_duration) #rename column
  weather_daylight$daylight_hrs <- weather_daylight$daylight_sec / 3600 #create daylight by hour column
  weather_daylight$city <- i # add city column
  weather_daylight$year <- substr(weather_daylight$date, 1,4) #create year column
  
  weather_daylight <- weather_daylight %>% 
    mutate(week = week(ymd(date))) %>% #add week column
    group_by(week) %>%  
    mutate(weekavg_hrs = mean(daylight_hrs)) #compute weekly average daylight hours

  weather_daylight <- weather_daylight[, col_order] #reorder columns
  daylight_dataframes[[i]] <- weather_daylight #add new data frame to list
}
```

```{r, echo=FALSE}
head(daylight_dataframes)
```

Snowfall is a numeric variable indicating the total amount of snowfall in millimeters for each day. The average snowfall between 2018 and 2023 was 0.26 millimeters with a maximum of 25.55 millimeters.

```{r, include = FALSE}
# Snowfall Data

# Test code with one city first
weather_snow <- weather_history("Detroit",
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "snowfall_sum") # unit is inches

# Check data
str(weather_snow)
head(weather_snow)
```

```{r, include = FALSE}
# Create loop to grab all cities of interest for snow fall

# First, create list of cities of interest
col_order_snow2 <- c("date", "week", "city", "daily_snow", "weekavg_snow")

# Create object to store each city as a data frame
snow_dataframes <- list()

# Use weather_history() to get historical weather data
for(i in cities) {
weather_snow <- weather_history(i,
  start = "2017-12-01",
  end = "2023-12-01",
  daily = "snowfall_sum")

  weather_snow <- weather_snow %>% 
    rename(daily_snow = daily_snowfall_sum) #rename column
  weather_snow$city <- i # add city column
  weather_snow$year <- substr(weather_snow$date, 1,4) #create year column
  
  weather_snow <- weather_snow %>% 
    mutate(week = week(ymd(date))) %>% #add week column
    group_by(week) %>%  
    mutate(weekavg_snow = mean(daily_snow)) #compute weekly average daylight hours

  weather_snow <- weather_snow[, col_order_snow2] #reorder columns
  snow_dataframes[[i]] <- weather_snow #add new data frame to list
}

head(snow_dataframes)
```

```{r, include = FALSE}
#Combining city data

# Combine all city dataframes into one long dataframe
daylight_long <- bind_rows(daylight_dataframes)
head(daylight_long)

# Combine all city dataframes into one long dataframe
snow_long <- bind_rows(snow_dataframes)
head(snow_long)
```

The average daily daylight did not varied very little by year; while there was slightly more variation in snowfall by year, it did not vary by more than 0.06 millimeters.

```{r, include = FALSE}
# Combining daylight and snowfall data
weather <- merge(daylight_long, snow_long, by = c("date", "week", "city"))
head(weather)
colnames(weather) 
```

```{r, echo=FALSE}
head(weather)
```

###Google trends

In order to capture SAD-related indicators over time, Google search frequency trends data was obtained through the R package "gtrendsR" for December 1, 2017 to December 1, 2023 from the following cities to represent different parts of Michigan (excluding the Upper Peninsula): Ann Arbor, Detroit, Lansing, Grand Rapids, and Kalamazoo. **rows... (??????)**

```{r, include=FALSE}
# google trends data

mich.trends <- read.csv("/Users/akarioya/temp/SURV727/Final Project/17_23_SearchFreqeuncies.csv", skip = 2) #allows third row to be column names
colnames(mich.trends)
```

```{r, include=FALSE}
# Clean gtrends whole Michigan data to match weather data
# Reorder date
mich.trends <- mich.trends %>% 
  mutate(date = as.Date(Week, format = "%m/%d/%y")) %>% 
  select(-c(Week)) %>% #remove old Week column
  mutate(year = substr(date, 1,4)) %>% 
  rename(light_therapy = light.therapy...Michigan.) %>% 
  rename(winter_blues = winter.blues...Michigan.) %>% 
  rename(SAD = Seasonal.affective.disorder...Michigan.) %>% 
  relocate(date, .before = light_therapy) #reorder columns

# Add week column that indicate number of weeks in year
mich.trends <- mich.trends %>% 
  mutate(week = week(ymd(date))) %>% 
  group_by(week)
```

```{r, echo=FALSE}
head(mich.trends)
```

```{r, include=FALSE}
# Combining whole Michigan gtrends data to weather data

weather_nd <- weather[, -1] #remove date column so data can merge smoothly

# Merge datasets based on variables
mich_df <- merge(mich.trends, weather_nd, by = c("week", "year"))
head(mich_df)
colnames(mich_df) 
```

## Analysis:

### Weather Data

We created interactive multi-line plots using both plotly and ggplot2 packagea in R. The key rationale behind the usage of these interactive data visualizations is to grant users the flexibility to navigate and scrutinize the weather pattern within their desired time span more intimately. We started with the '**Average Weekly Snowfall from 12/1/2017- 12/1/2023**' and '**Average Weekly Daylight from 12/1/2017- 12/1/2023**' to examine how snowfall pattern and daylight durations change throughout our selected period for selected cities in Michigan.

```{r, echo=FALSE}

library(ggplot2)
library(plotly)

p1 <- ggplot(data = weather, aes(x= date))+
  geom_line(aes(y = weekavg_snow, color = city)) +
  ggtitle("Average Weekly Snowfall 12/1/2017- 12/1/2023") + xlab("Week") + ylab("Snowfall (mm)") +
  theme(plot.title = element_text(hjust = 0.5))
  
# Create a ggplotly object from the ggplot object
p1 <- ggplotly(p1)

# Print the interactive plot
print(p1)

p2 <- ggplot(data = weather, aes(x= date))+
  geom_line(aes(y = weekavg_hrs, color = city)) +
  ggtitle("Average Weekly Daylight 12/1/2017- 12/1/2023") + xlab("Week") + ylab("Daylight (hours)") +
  theme(plot.title = element_text(hjust = 0.5))
  
# Create a ggplotly object from the ggplot object
p2 <- ggplotly(p2)

# Print the interactive plot
print(p2)
```

### Google Search Frequencies

```{r, include=FALSE}
# For Google Trends data

# Query keywords
library(gtrendsR)
SADtrends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI", 
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
plot(SADtrends)

# Grab each dataset and save as CSV in case we have issues querying again
# interest by city
trends_city <- SADtrends$interest_by_city
write.csv(rest_city, "SADtrends_interest_by_city.csv")

# interest over time
trends_time <- SADtrends$interest_over_time
write.csv(trends_time, "SADtrends_interest_over_time.csv")

# interest by dma (seems like certain big cities)
trends_dma <- SADtrends$interest_by_dma
write.csv(trends_dma, "SADtrends_interest_by_dma.csv")

# interest by dma (seems like certain big cities)
trends_related <- SADtrends$related_queries
write.csv(trends_related, "SADtrends_related_queries.csv")
```


```{r, include=FALSE}
# Examining interest over city data

# Reshape data so each keyword becomes a column and each row is a city
city_ranking <- trends_city %>%
  pivot_wider(names_from = keyword, 
              values_from = hits)
```

Initially, we lookat Interest by city data (shown below) and we find that this data seems not to have a lot of data points.

```{r, echo=FALSE}
# See data sorted by hits
head(sort(city_ranking$`light therapy`, decreasing = TRUE))
head(sort(city_ranking$`seasonal affective disorder`, decreasing = TRUE)) # this column seems to be the only one with data points
head(sort(city_ranking$`winter blues`, decreasing = TRUE))
```

Therefore, we try another method, to query each major city separately. There is no manual for which geo code to use for Michigan cities, but the URL on the gtrends website after selecting the filters displays a geo code. We endup using this to see if it will pick up more specific data (For example: https://trends.google.com/trends/explore?date=2017-12-01%202023-12-01&*geo=US-MI*-540&q=light%20therapy&hl=en-US).

```{r, include=FALSE}
detroit_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-505", # geo code for Detroit from gtrends website URL 
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
detroit_trends <- as_tibble(detroit_trends$interest_over_time) 
write.csv(detroit_trends, "SADtrends_Detroit.csv") # save as csv

head(detroit_trends$interest_over_time)
write.csv(detroit_trends$interest_over_time, "SADtrends_Detroit.csv") # save as csv
```

```{r, include=FALSE}
#Repeat with other cities (creating a loop gave me the connection error message)

# List of city geo codes taken from gtrends URL
# Lansing = "US-MI-551"
# Alpena = "US-MI-583" - did not have any data, excluded
# Flint-Saginaw-BayCity = "US-MI-513"
# GrandRapids-Kalamazoo-BattleCreek = "US-MI-563"
# Marquette = "US-MI-553"
# Traverse City-Cadillac = "US-MI-540"

lansing_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-551",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
lansing_trends <- as_tibble(lansing_trends$interest_over_time) 
write.csv(lansing_trends, "SADtrends_Lansing.csv") # save as csv

flint_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-513",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
flint_trends <- as_tibble(flint_trends$interest_over_time)
write.csv(flint_trends, "SADtrends_Flint-Saginaw-BayCity.csv")

gr_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-563",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
gr_trends <- as_tibble(gr_trends$interest_over_time)
write.csv(gr_trends, "SADtrends_GrandRapids-Kalamazoo-BattleCreek.csv") # save as csv

marq_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-553",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
marq_trends <- as_tibble(marq_trends$interest_over_time)
write.csv(marq_trends, "SADtrends_Marquette.csv") # save as csv

trav_trends <- gtrends(c("light therapy", "seasonal affective disorder", "winter blues"), 
               geo = "US-MI-540",  
               time = "2017-12-01 2023-12-01", 
               low_search_volume = TRUE)
trav_trends <- as_tibble(trav_trends$interest_over_time)
write.csv(trav_trends, "SADtrends_Traverse City-Cadillac.csv") # save as csv

# Combine all into one dataset
SAD_bycity <- bind_rows(detroit_trends, lansing_trends, alpena_trends, flint_trends, gr_trends, marq_trends, trav_trends)
summary(SAD_bycity)
write.csv(SAD_bycity, "SADtrends_bycity.csv") # save as csv
```

```{r, include=FALSE}
# Clean data
SAD_bycity_grp <- SAD_bycity %>% 
  mutate(date = as.Date(date, format = "%Y-%m/%d")) %>% 
  select(-c(time, gprop, category, interest_by_city)) %>% #remove unnecessary variables
  mutate(year = substr(date, 1,4)) %>% #create year variable
  mutate(month = substr(date,6,7)) %>% #create month variable  
  group_by(date, geo, year, month, keyword) %>%
  summarise(hits = sum(hits)) %>% 
  ungroup() %>% 
  rename(city = geo)

# Recode city values
SAD_bycity_grp$city <- recode(SAD_bycity_grp$city, 
                              "US-MI-505" = "Detroit",
                              "US-MI-551" = "Lansing",
                              "US-MI-513" = "Flint-Saginaw-Bay City",
                              "US-MI-563" = "Grand Rapids-Kalamazoo-Battle Creek",
                              "US-MI-553" = "Marquette",
                              "US-MI-540" = "Traverse City-Cadillac")

# Make keywords into columns with hits as values
SAD_bycity_grp <- SAD_bycity_grp %>%
  spread(key = keyword, value = hits)
head(SAD_bycity_grp)
```

Unlike looking at the overall weather pattern across our period of interest using interactive visualizations, we specifically looked at "Monthly freqeuncies for SAD-related search hits in 2018" using a static muti-line plot here to explore the monthyl pattern about how people from different Michigan cities search for SAD-related terms in 2018.
```{r, echo=FALSE}
SAD_bycity_grp %>% ggplot(aes(x= date)) +
  geom_line(aes(y = `light therapy`, color = "Light Therapy"), linewidth = 1.2)+
  geom_line(aes(y = `winter blues`, color = "Winter Blues"), linewidth = 1.2) +
  geom_line(aes(y = `seasonal affective disorder`, color = "Seasonal Affective Disorder"), linewidth = 1.2) + 
  scale_x_date(limits = as.Date(c("2018-01-01","2018-12-31"))) +
  ggtitle("Monthly freqeincies for SAD-related search hits in 2018") + xlab("Month") + ylab("Search hits")
```

### Weather Data + Google Search Frequencies
Finally, we created another interactive visualization us through shiny package in R. This interactive map is centered at the coordinates for Michigan and places markers at locations based on longitude and latitude. When a user clicks on a marker, a popup will display with information about the date, city, average daylight, snowfall, and search frequencies for SAD-related terms.
```{r, echo=FALSE}
#Trying Shiny app

library(shiny)
library(rsconnect)
library(leaflet)

# Shiny UI
ui <- fluidPage(
  titlePanel("Interactive Map"),
  leafletOutput("map")
)

# Server
server <- function(input, output) {
  
  output$map <- renderLeaflet({
    leaflet() %>%
      addTiles() %>%
      setView(lng = -83.7430, lat = 44.3148, zoom = 7) %>%  # Coordinates for Michigan
      addMarkers(data = mich_df, ~longitude, ~latitude,
                 popup = paste("Date: ", mich_df$date, "<br>",
                               "City: ", mich_df$city, "<br>",
                               "Weather: ", mich_df$average_daily_daylight, " hours of daylight, ",
                               mich_df$average_daily_snow, " inches of snow", "<br>",
                               "Trends: Light Therapy - ", mich_df$light_therapy, ", ",
                               "SAD - ", mich_df$seasonal_affective_disorder, ", ",
                               "Winter Blues - ", mich_df$winter_blues))
  })
}

shinyApp(ui, server)
```

## Conclusion:

### Findings:


### Limitations: 

The current analysis has some limitations. One concern is unknown confounding variables that may play a large role in the relationship between daylight, snowfall, and changes in the public's interest of SAD-related topics reflected by Google search trends. It important to note that we adopted common terms assaociated with SAD as our search keys for Google trend, so it is not a comprehensive lists of SAD-related terms. Although some research [@hoang2011association] suggesting that vitamin D synthesis in the body from sunlight affects or improves one's mood has gained popularity in recent years, supporting our analysis of Google search trends and weather data, the intent behind users' searches are not clear. Furthermore, higher or lower search hits do not directly reflect the actual number of diagnoses or people experiencing SAD-related symptoms. Thus, we cannot make strong or holistic conclusions about the link between weather and increase in SAD interest or diagnoses. A second limitation is the nature of the Google trends data. Search trend hits were aggregated by week while hourly data was available for weather. Higher granulated data of search hits may reveal different patterns. Moreover, search hit frequencies were low for many cities and days which lead to missing data. Different sets of keywords may have better success in gathering more data and uncovering different information, but the task of choosing the appropriate keywords is challenging. Since mood is a factor in the link between weather and SAD, an additional analysis such as a sentiment analysis of Internet users' mood over the year may provide more nuance in our conclusions.

## References: